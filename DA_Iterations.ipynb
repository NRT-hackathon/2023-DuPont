{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09cdb6e",
   "metadata": {},
   "source": [
    "### DuPont Mixture Design - Inverse Modeling\n",
    "\n",
    "## Alison Shapiro, Sean Farrington, and Peter Osazuwa\n",
    "\n",
    "***Machine Learning Techniques Used:*** \n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba6cd2",
   "metadata": {},
   "source": [
    "## Script for Dual Annealing Optimization\n",
    "\n",
    "#### Utilize predictions from the Gaussian process regression\n",
    "\n",
    "Start by remaking the Gaussian process regression code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf080d15",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e810d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd180414",
   "metadata": {},
   "source": [
    "### Import new packages for GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbf62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import dual_annealing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, RBF, WhiteKernel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8c777",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a86c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'DATA/training_inputs.xlsx'\n",
    "\n",
    "df = pd.read_excel(file)\n",
    "\n",
    "design = ['Powdered Additive','Base Resin A','Base Resin B','Stabilizer','Temperature','Screw Speed (RPM)']\n",
    "performance = ['Toughness (J/m2)','Modulus (GPa)']\n",
    "\n",
    "X = df[design]\n",
    "y = df[performance]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(data=X,\n",
    "                columns=design)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e5894",
   "metadata": {},
   "source": [
    "### Create model \n",
    "\n",
    "***Need two separate models for each output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b7419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = C() + RBF(length_scale=np.ones(X.shape[1]))\n",
    "\n",
    "# Split to two models\n",
    "reg_0 = GaussianProcessRegressor(kernel=kernel,random_state=1773).fit(X,y[performance[0]])\n",
    "reg_1 = GaussianProcessRegressor(kernel=kernel,random_state=1773).fit(X,y[performance[1]])\n",
    "\n",
    "y_pred_0 = reg_0.predict(X)\n",
    "y_pred_1 = reg_1.predict(X)\n",
    "y_pred_GPR = pd.DataFrame({performance[0]:y_pred_0,\n",
    "                       performance[1]:y_pred_1\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44fcd0",
   "metadata": {},
   "source": [
    "# Particle Swarm Routine\n",
    "\n",
    "In this inverse model we will try to predict a data point from the testing set\n",
    "\n",
    "The values for this datum is as follows:\n",
    "\n",
    "- Powdered Additive: 0.32\n",
    "- Base Resin A: 0.6\n",
    "- Base Resin B: 0.05\n",
    "- Stabilizer: 0.09\n",
    "- Temperature: 406\n",
    "- Screw Speed (RPM): 120\n",
    "\n",
    "\n",
    "- Toughness (J/m2): 656\n",
    "- Modulus (GPa): 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ef8a2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "from pyswarms.utils.plotters import (plot_cost_history, plot_contour, plot_surface)\n",
    "\n",
    "X_true = np.array([0.32,0.6,0.05,0.09,406,120])\n",
    "y_true = np.array([656,5.1])\n",
    "\n",
    "def CostFuncSlack(x,a,b):\n",
    "    \"\"\"\n",
    "    Input 'x' is an array of unscaled variables. For this function they are:\n",
    "    \n",
    "    x = ['Powdered Additive','Base Resin A','Base Resin B','Temperature','Screw Speed']\n",
    "    \n",
    "    Notice that 'Stabilizer' is removed from this array, this must be accounted for so \n",
    "    scaling is done properly.\n",
    "    \n",
    "    Use 'y_0_target' and 'y_1_target' to assign the desired performance\n",
    "    \n",
    "    The cost function uses the slack variable approach to reduce\n",
    "    dimensionality and ensure the composition is real.\n",
    "    \n",
    "    The barrier term here is a large penalty associated with the first three components \n",
    "    being greater than 1\n",
    "    \n",
    "    \"\"\"\n",
    "    y_0_target = a # Target toughness (J/m2)\n",
    "    y_1_target = b # Target Modulus (GPa)\n",
    "    \n",
    "    sum_noslack = x[0]+x[1]+x[2]\n",
    "    \n",
    "    slack = 1 - (sum_noslack)\n",
    "    \n",
    "    x_full = np.insert(x,3,slack) # Insert slack into the fourth position\n",
    "    \n",
    "    x_scaled = scaler.transform(x_full.reshape(1,-1))\n",
    "    \n",
    "    y_0,std_0 = reg_0.predict(x_scaled,return_std=True)\n",
    "    y_1,std_1 = reg_1.predict(x_scaled,return_std=True)\n",
    "    \n",
    "    var_0 = std_0**2/y_0_target # Normalized variance\n",
    "    var_1 = std_1**2/y_1_target # Normalized variance\n",
    "    \n",
    "    y_0_penalty = ((y_0-y_0_target)/y_0_target)**2\n",
    "    y_1_penalty = ((y_1-y_1_target)/y_1_target)**2\n",
    "    \n",
    "    barrier = 0\n",
    "    if sum_noslack >= 1:\n",
    "        barrier = 1e6\n",
    "            \n",
    "    loss = y_0_penalty + y_1_penalty + var_0 + var_1 + barrier\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895ff33",
   "metadata": {},
   "source": [
    "# Ten iterations of dual annealing for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d467b9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set-up Bounds\n",
    "max_bound = [1,1,1,475,121]\n",
    "min_bound = [0,0,0,380,79]\n",
    "\n",
    "file = 'ANALYSIS/Iterations_DA.xlsx'\n",
    "df = pd.DataFrame()\n",
    "df.to_excel(file,index=False)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Iteration {i}')\n",
    "    \n",
    "    # Perform optimization\n",
    "    ret = dual_annealing(CostFuncSlack,\n",
    "                        bounds=list(zip(min_bound,max_bound)),\n",
    "                        args = (y_true[0],y_true[1]),\n",
    "                        maxiter = 5_000)\n",
    "    \n",
    "    pos = ret.x\n",
    "    Stabilizer = 1 - (pos[0]+pos[1]+pos[2])\n",
    "    pos = np.insert(pos,3,Stabilizer)\n",
    "\n",
    "    pos_scaled = scaler.transform(pos.reshape(1,-1))\n",
    "    y_pred_0 = reg_0.predict(pos_scaled)\n",
    "    y_pred_1 = reg_1.predict(pos_scaled)\n",
    "    \n",
    "    df1 = pd.read_excel(file)\n",
    "    df2 = pd.DataFrame({'Cost':ret.fun,\n",
    "                        design[0]:pos[0],\n",
    "                        design[1]:pos[1],\n",
    "                        design[2]:pos[2],\n",
    "                        design[3]:pos[3],\n",
    "                        design[4]:pos[4],\n",
    "                        design[5]:pos[5],\n",
    "                        performance[0]:y_pred_0,\n",
    "                        performance[1]:y_pred_1\n",
    "                        })\n",
    "    \n",
    "    df = pd.concat([df1,df2])\n",
    "    df.to_excel(file,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
